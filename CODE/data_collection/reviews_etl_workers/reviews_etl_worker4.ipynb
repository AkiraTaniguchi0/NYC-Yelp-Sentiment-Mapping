{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b67e1b1",
   "metadata": {},
   "source": [
    "## Reviews Scraper | Worker 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f0a6e",
   "metadata": {},
   "source": [
    "**Step 1:** Set variables for offset, batch size, and worker file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48515f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### UPDATE ###################################\n",
    "OFFSET = 765*3\n",
    "BATCH = 765\n",
    "worker_reviews_csv = 'reviews_worker_files/worker4/reviews_w4.csv'\n",
    "worker_reviews_error_csv = 'reviews_worker_files/worker4/reviews_error_w4.csv'\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5255c4",
   "metadata": {},
   "source": [
    "**Step 2:** Worker functions. Modified version of `reviews_etl.py`. \n",
    "\n",
    "<font color='red'>**### ATTENTION: Only run the below cell ONCE! ###**</font>\n",
    "\n",
    "Running multiple times will affect the `GLOBAL_COUNTER` variable and lead to inaccurate \"Remaining businesses to scrape\" metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3191a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import beepy\n",
    "# import pyspark\n",
    "# from pyspark.sql.functions import col, isnull\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "############ INSERT API KEY HERE ############\n",
    "API_KEY = ''\n",
    "#############################################\n",
    "GLOBAL_COUNTER = 0\n",
    "\n",
    "\n",
    "def apiURL(yelp_url):\n",
    "    global API_KEY\n",
    "    return f\"https://api.scrapingdog.com/scrape?api_key={API_KEY}&url={yelp_url}&dynamic=false\"\n",
    "\n",
    "def apiCall(yelp_url):\n",
    "    try:\n",
    "        return requests.get(apiURL(yelp_url), timeout=120).text\n",
    "    except Exception as e:\n",
    "        print(f'API connection error detected: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def convertToJson(response):\n",
    "    review_list = json.loads(\n",
    "        response[response.find('\\\"review\\\":[')+9:response.find('],\\\"aggregateRating\\\"')+1]\n",
    "    )\n",
    "    return review_list\n",
    "\n",
    "def buildDf(yelp_id, review_list):\n",
    "    author_list = [r['author'] for r in review_list] \n",
    "    datePublished_list = [r['datePublished'] for r in review_list] \n",
    "    reviewRating_list = [r['reviewRating']['ratingValue'] for r in review_list] \n",
    "    reviewText_list = [r['description'] for r in review_list]\n",
    "    \n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'id': [yelp_id for i in range(len(review_list))],\n",
    "            'author': author_list,\n",
    "            'rating': reviewRating_list,\n",
    "            'text': reviewText_list\n",
    "        }\n",
    "    )\n",
    "\n",
    "def getIdsAndUrls():\n",
    "    global OFFSET, BATCH, GLOBAL_COUNTER, worker_reviews_csv, worker_reviews_error_csv\n",
    "    \n",
    "    business_10plus_df = pd.read_csv('csv_files/business_10plus.csv', usecols = ['id','url'])\n",
    "\n",
    "    # Truncate URL and remove any businesses where id = #NAME?\n",
    "    business_10plus_df['url'] = business_10plus_df['url'].apply(lambda u: u[0:u.find('?')])\n",
    "    business_10plus_df = business_10plus_df[business_10plus_df['id'] != '#NAME?']\n",
    "\n",
    "    # Read in distinct ids in review.csv\n",
    "    reviews_df = pd.read_csv(worker_reviews_csv, usecols = ['id', 'rating']).drop_duplicates(['id'])\n",
    "\n",
    "    # Read in ids in review_error.csv\n",
    "    reviews_error_df = pd.read_csv(worker_reviews_error_csv)\n",
    "    reviews_error_df['error_flag'] = 1\n",
    "\n",
    "    # Only select ids and urls that don't exist in reviews.csv or review_error.csv\n",
    "    joined_df = business_10plus_df \\\n",
    "                    .merge(reviews_df, left_on='id', right_on='id', how='left') \\\n",
    "                    .merge(reviews_error_df, left_on='id', right_on='id', how='left')\n",
    "\n",
    "\n",
    "    joined_df = joined_df[joined_df['rating'].isna() & joined_df['error_flag'].isna()] \\\n",
    "                    .drop(columns=['rating', 'error_flag']) \\\n",
    "                    .sort_values('id') \\\n",
    "                    .reset_index(drop=True)\n",
    "\n",
    "    # Return list of id-url pairs\n",
    "    return [joined_df.loc[i,:].values.flatten().tolist() for i in range(OFFSET, OFFSET+BATCH-GLOBAL_COUNTER)]\n",
    "\n",
    "\n",
    "def main(business_count=10):\n",
    "    global GLOBAL_COUNTER\n",
    "    \n",
    "    # Counters for final report out\n",
    "    success_counter = 0\n",
    "    error_counter = 0\n",
    "    \n",
    "    # Get list of ids and urls for API calls\n",
    "    id_url_list = getIdsAndUrls()[0:BATCH-GLOBAL_COUNTER]\n",
    "    \n",
    "    # Read reviews.csv\n",
    "    reviews_df = pd.read_csv(worker_reviews_csv)\n",
    "    \n",
    "    # Read reviews_error.csv\n",
    "    reviews_error_df = pd.read_csv(worker_reviews_error_csv)\n",
    "\n",
    "    for b in id_url_list[0: min(business_count, len(id_url_list))]:\n",
    "        print(f'START: Collect review data for id {b[0]} ({success_counter+error_counter+1}/{min(business_count, len(id_url_list))})')\n",
    "        try:    \n",
    "            response = apiCall(yelp_url=b[1])\n",
    "            review_list = convertToJson(response=response)\n",
    "            response_df = buildDf(yelp_id=b[0], review_list=review_list)\n",
    "\n",
    "            reviews_df = pd.concat([reviews_df, response_df])\n",
    "            reviews_df.to_csv(worker_reviews_csv, index=False)\n",
    "            print(f'SUCCESS: Review data for id {b[0]} successfully written to CSV')\n",
    "\n",
    "            success_counter+=1\n",
    "        except:\n",
    "            print(f'*** ERROR: Review data not collected for id {b[0]} ***')\n",
    "            reviews_error_df = pd.concat([reviews_error_df, pd.DataFrame([b[0]], columns=['id'])])\n",
    "            reviews_error_df.to_csv(worker_reviews_error_csv, index=False)\n",
    "\n",
    "            error_counter+=1\n",
    "    \n",
    "    \n",
    "    GLOBAL_COUNTER = GLOBAL_COUNTER + success_counter + error_counter\n",
    "    print('################# COMPLETE #################') \n",
    "    print(f'API call for {min(business_count, len(id_url_list))} businesses complete')\n",
    "    print(f'- Successes: {success_counter}  - Errors: {error_counter}')\n",
    "    print(f'- Remaining businesses to scrape: {len(id_url_list) - min(business_count, len(id_url_list))}')\n",
    "    print('############################################') \n",
    "    beepy.beep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade05296",
   "metadata": {},
   "source": [
    "**Step 3:** Run `main()` to begin scraping. `business_count` variable specifies how many pages to scrape before terminating the `main()` (Default value is 10). `main()` can be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52c9c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: Fri Oct 27 20:30:05 2023\n",
      "################# COMPLETE #################\n",
      "API call for 0 businesses complete\n",
      "- Successes: 0  - Errors: 0\n",
      "- Remaining businesses to scrape: 0\n",
      "############################################\n",
      "end: Fri Oct 27 20:30:10 2023\n"
     ]
    }
   ],
   "source": [
    "print('start:',time.ctime(time.time()))\n",
    "main(250)\n",
    "print('end:',time.ctime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd2a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
